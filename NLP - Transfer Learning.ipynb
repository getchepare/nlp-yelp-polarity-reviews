{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base packages\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Import DL packages\n",
    "import tensorflow as tf\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Keras version: {tf.keras.__version__}')\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train/test datasets\n",
    "Create a function to read train and test datasets with follow actions:\n",
    "- Have a look at readme.txt from the unzipped folder to get more information about the datasets\n",
    "- Columns should be renamed to 'rate' and 'text'\n",
    "- Take a random sample of 5000 records for training and test datasets\n",
    "- Positive labels should be mapped to 0 (instead of 2 in the initial dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_format_dataset(dataset_path):\n",
    "    \n",
    "    return()\n",
    "\n",
    "train_dataset_path = 'path_to_train_data'\n",
    "test_dataset_path = 'path_to_test_data'\n",
    "train_data = read_format_dataset(train_dataset_path)\n",
    "test_data = read_format_dataset(test_dataset_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your Keras model using Transfer Learning\n",
    "Now you should define your NN structure using Keras sequential layers. Your base model will come from TensorFlow Hub with source url https://tfhub.dev/google/universal-sentence-encoder/4.\n",
    "To import this base model in your structure, you must use hub.KerasLayer function (https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer). You should add following parameters:\n",
    "- input_shape = []\n",
    "- dtype = tf.string\n",
    "- trainable = False\n",
    "\n",
    "Please note this trainable option that allows to retrain the entire NN or not.\n",
    "\n",
    "Now that you have your base model, you should add a new layer on top to predict a probability for our 2 classes (Positive/Negative). Which layer would you use for this ? Which activation function ?\n",
    "\n",
    "Your final model should have two layers:\n",
    "- base model with 256797824 params\n",
    "- prediction layer with 513 params\n",
    "\n",
    "Please have a look at the number of training params and its relation with base model option trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compiler\n",
    "Compile your Keras model using an Adam optimizer, binary crossentropy for the loss and accuracy as the target metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Split your training data into x_train, x_valid, y_train, y_valid using sklearn function. Test size must be set to 0.3 and the repartition of the target variable should be similar between your valid and training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train your NN by providing your training and valid datasets. Number of epochs can be set to 5 for now. You need to save the model fit output into history variable, so we can plot the loss later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/validation error history\n",
    "You can plot the training/validation error and accuracy using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    \"\"\"Plot training and (optionally) validation loss and accuracy\"\"\"\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs, loss, '.--', label='Training loss')\n",
    "    final_loss = loss[-1]\n",
    "    title = 'Training loss: {:.4f}'.format(final_loss)\n",
    "    plt.ylabel('Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        val_loss = history.history['val_loss']\n",
    "        plt.plot(epochs, val_loss, 'o-', label='Validation loss')\n",
    "        final_val_loss = val_loss[-1]\n",
    "        title += ', Validation loss: {:.4f}'.format(final_val_loss)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs, acc, '.--', label='Training acc')\n",
    "    final_acc = acc[-1]\n",
    "    title = 'Training accuracy: {:.2f}%'.format(final_acc * 100)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        plt.plot(epochs, val_acc, 'o-', label='Validation acc')\n",
    "        final_val_acc = val_acc[-1]\n",
    "        title += ', Validation accuracy: {:.2f}%'.format(final_val_acc * 100)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset\n",
    "Compute the accuracy for our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ...\n",
    "acc = ...\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st interpretation\n",
    "Apart from the final precision, what do you think about the training/validation curves ? Is there any evidence of overfitting when we freeze the base layer ?\n",
    "\n",
    "Now, you can go back to the model definition and unfreeze our base layers. You should see the difference with the number of trainable parameters. No other parameters should be updated. Obviously training your model will take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd interpretation\n",
    "What do you notice now with our training/validation curves? Do you have any hints about why we observe such results ?\n",
    "\n",
    "You need to find a way to solve this problem. Please go back to the model definition and try to add new type of layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}